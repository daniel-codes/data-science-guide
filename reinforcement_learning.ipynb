{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "A (finite) Markov Decision Process (MDP) is defined by:\n",
    "- a (finite) set of states $(\\mathcal{S})$ *\n",
    "- a (finite) set of actions $(\\mathcal{A})$ *\n",
    "- a (finite) set of rewards $(\\mathcal{R})$ $^\\%$\n",
    "- the one-step dynamics of the environment: $^\\%$\n",
    "   $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$\n",
    "- a discount rate $\\gamma \\in [0,1]$ *\n",
    "\n",
    "$^*$Agent knows   \n",
    "$^\\%$Agent does not know   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Setting\n",
    "\n",
    "The reinforcement learning (RL) framework is characterized by an **agent** learning to interact with its **environment**.\n",
    "\n",
    "At each time step, the agent receives the environment's **state** *(the environment presents a situation to the agent)*, and the agent must choose an appropriate **action** in response. One time step later, the agent receives a **reward** *(the environment indicates whether the agent has responded appropriately to the state)* and a new **state**.\n",
    "\n",
    "All **agents** have the goal to maximize expected **cumulative reward**, or the expected sum of rewards attained over all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic vs. Continuing Tasks\n",
    "A **task** is an instance of the reinforcement learning (RL) problem.\n",
    "Three primary signals are sent between the environment and the agent:   \n",
    "- State $S_t$:  environment -> $S_t$ -> Agent\n",
    "- Action $A_t$:  Agent -> $A_t$ -> environment\n",
    "- Reward $R_t$:  environment -> $R_t$ -> Agent\n",
    "\n",
    "#### Continuing Task\n",
    "Definition: tasks that continue forever, without end. \n",
    "$S_0, A_0, R_1, S_1, A_1, ...$\n",
    "\n",
    "#### Episodic Task  \n",
    "Definition: tasks with a well-defined starting and ending point at some time step $T$    \n",
    "$S_0, A_0, R_1, S_1, A_1, ... , R_T, S_T$\n",
    "\n",
    "In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**. Episodic tasks come to an end whenever the agent reaches a **terminal state**. An example is an agent playing chess where the reward is only delivered at the end of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Goals and Rewards\n",
    "\n",
    "Reward Hypothesis: All goals can be framed as the maximization of **expected** cumulative reward. \n",
    "\n",
    "The \"return\" at time step *t* is:  \n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} +...$\n",
    "\n",
    "At time step *t*, the agent selects actions $A_t$ with the goal maximizing expected return $G_t$.\n",
    "\n",
    "#### Discount Return\n",
    "\n",
    "Note that \"return\" and \"discounted return\" are often used interchangably. \n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +...= \\sum^\\infty_{k=0} \\gamma^k R_{t+k+1}$ where $\\gamma \\in [0,1] $\n",
    "\n",
    "discount rate $\\gamma$. As $\\gamma$ approaches 0, only the immediate reward is favored. \n",
    "\n",
    "#### Discount Rate\n",
    "The discount rate $\\gamma$ is something that you set, to refine the goal that you have the agent.\n",
    "- It must satisfy $0 \\leq \\gamma \\leq 1$.\n",
    "- If $\\gamma=0$, the agent only cares about the most immediate reward.\n",
    "- If $\\gamma=1$, the return is not discounted.\n",
    "- For larger values of $\\gamma$, the agent cares more about the distant future. Smaller values of $\\gamma$ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.\n",
    "\n",
    "It is common to set the discount rate to 0.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDPs and One-Step Dynamics\n",
    "\n",
    "\n",
    "- In general, the state space $\\mathcal{S}$ is the set of all nonterminal states.\n",
    "- In continuing tasks (like the recycling task detailed in the video), this is equivalent to the set of all states.\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of all states, including terminal states.\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions available to the agent.\n",
    "\n",
    "In the event that there are some states where only a subset of the actions are available, we can also use $\\mathcal{A}(s)$ to refer to the set of actions available in state $s\\in\\mathcal{S}$.  \n",
    "\n",
    "At an arbitrary time step t, the agent-environment interaction has evolved as a sequence of states, actions, and rewards  \n",
    "\n",
    "\n",
    "$(S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t)$\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, it considers only at the state and action at the previous time step $(S_t, A_t)$ \n",
    "\n",
    "In particular, it does not care what state was presented to the agent more than one step prior. (In other words, the environment does not consider any of $\\{ S_0, \\ldots, S_{t-1} \\}$).\n",
    "\n",
    "And, it does not look at the actions that the agent took prior to the last one. (In other words, the environment does not consider any of $\\{ A_0, \\ldots, A_{t-1} \\}$)\n",
    "\n",
    "Furthermore, how well the agent is doing, or how much reward it is collecting, has no effect on how the environment chooses to respond to the agent. (In other words, the environment does not consider any of $\\{ R_0, \\ldots, R_t \\}$).\n",
    "\n",
    "Because of this, we can completely define how the environment decides the state and reward by specifying\n",
    "\n",
    "$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$\n",
    "\n",
    "for each possible $s', r, s, \\text{and } a$. These conditional probabilities are said to specify the one-step dynamics of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies\n",
    "A policy determines how an agent chooses an action in response to the current state. In other words, it specifies how the agent responds to situations that the environment has presented.\n",
    "\n",
    "- A deterministic policy is a mapping $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$\n",
    "\n",
    "- A stochastic policy is a mapping $\\pi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$   \n",
    "$\\pi(a|s) = \\mathbb{P}(A_t = a | S_t = s)$   \n",
    "*stochasiticity = randomness*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Value Functions\n",
    "\n",
    "For each state, the **state-value function** yields the expected return, if the agenst started in that state, and then followed the policy for all time steps.\n",
    "\n",
    "Definition: We call $\\upsilon_{\\pi}$ the **state-value function** for policy $\\pi$. The value of state **S** under a policy $\\pi$ is\n",
    "$\\upsilon_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t = s]$\n",
    "\n",
    "where for each **s**=state, it yields the $\\mathbb{E}_{\\pi}$=expected, $[G_t]$=return if the agent starts in state **s** and then uses the policy $\\pi$ to choose its actions for all time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bellman Equations\n",
    "In a gridworld example, once the agent selects an action, \n",
    "- it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and \n",
    "- the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution).\n",
    "\n",
    "In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state.\n",
    "\n",
    "More complicate however, where the reward rr and next state $\\mathcal{s}'$ are drawn from a (conditional) probability distribution $p(s',r|s,a)$, the Bellman Expectation Equation (for $v_{\\pi}$) expresses the value of any state $\\mathcal{s}$ in terms of the *expected* immediate reward and the *expected* value of the next state:\n",
    "\n",
    "$v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$ \n",
    "\n",
    "#### Deterministic Calculation\n",
    "\n",
    "In the event that the agent's policy $\\pi$ is deterministic, the agent selects action $\\pi(s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over two variables ($s'$ and $r$):\n",
    "\n",
    "$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s'))$ \n",
    "\n",
    "In this case, we multiply the sum of the reward and discounted value of the next state $(r+\\gamma v_\\pi(s'))$ by its corresponding probability $p(s',r|s,\\pi(s))$ and sum over all possibilities to yield the expected value.\n",
    "\n",
    "#### Stocastic Calculation\n",
    "\n",
    "If the agent's policy $\\pi$ is stochastic, the agent selects action $a$ with probability $\\pi(a|s)$ when in state $s$, and the Bellman Expectation Equation for the State-Value Function can be rewritten as the sum over three variables $(s'$, $r$, and $a$):\n",
    "\n",
    "$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "In this case, we multiply the sum of the reward and discounted value of the next state $(r+\\gamma v_\\pi(s'))$ by its corresponding probability $\\pi(a|s)p(s',r|s,a)$ and sum over all possibilities to yield the expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality\n",
    "\n",
    "A policy $\\pi'$ is defined to be better than or equal to a policy $\\pi$ if and only if $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all $s\\in\\mathcal{S}$.\n",
    "\n",
    "*Note: It is often possible to find two policies that cannot be compared.* \n",
    "\n",
    "An optimal policy $\\pi_*$ satisfies $\\pi_* \\geq \\pi$ for all policies $\\pi$. An optimal policy is guaranteed to exist but may not be unique. \n",
    "\n",
    "*Note: An optimal policy is guaranteed to exist, but may not be unique.* \n",
    "\n",
    "All optimal policies have the same state-value function $v_*$ called the optimal state-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Value Function\n",
    "\n",
    "- The **action-value function** for a policy $\\pi$ is denoted $q_\\pi$. For each state $s \\in\\mathcal{S}$ and action $a \\in\\mathcal{A}$, it yields the expected return if the agent starts in state $s$, takes action $a$, and then follows the policy for all future time steps. That is, $q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$. We refer to $q_\\pi(s,a)$ as the **value of taking action** $a$ in state $s$ under a policy $\\pi$ (or alternatively as the **value of the state-action pair** ($s, a$).\n",
    "- All optimal policies have the same action-value function $q_*$, called the **optimal action-value function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies\n",
    "\n",
    "Once the agent determines the optimal action-value function $q_*$ it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Iterative Policy Evaluation\n",
    "\n",
    "Bellman Expectation Equation for the State-Value Function\n",
    "\n",
    "$v_\\pi(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "Update Rule for Iterative Policy Evaluation\n",
    "\n",
    "$V(s) = \\text{} \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma V(s'))$\n",
    "\n",
    "where V is the current guess for the policy's value function.\n",
    "\n",
    "code would look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
