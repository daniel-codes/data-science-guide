{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "The three branches of machine learning regularly recognized today\n",
    "- Supervised Machine Learning\n",
    "- Unsupervised Machine Learning\n",
    "- Reinforcement Machine Learning\n",
    "\n",
    "In supervised machine learning our algorithms learn from labeled data. After studying labelled data, these techiques are able to determine which label should be given to new data based on observing patterns and associating those patterns to new unlabeled data. \n",
    "\n",
    "Supervised learning can be divided into two categories: \n",
    "- Classification\n",
    "- Regression\n",
    "\n",
    "Classification models predict a category that an item belongs to. \n",
    "Regression models predict a numeric value (e.g. home price).\n",
    "\n",
    "Unsupervised learning creates models for data when there are noe pre-existing labels to train on. \n",
    "\n",
    "Reinforcement Learning is used to train algorithms that learn based on taking certain actions and receiving rewards for those actions (e.g. self-driving cars, game playing agents). \n",
    "\n",
    "Deep learning can be applied to all three categories. Historically, we've cared less and less about how we make predictions, and more about the accuracy of our predictions. It has three major barriers to entry:\n",
    "1. You must have enough data to train on\n",
    "2. Computing power\n",
    "3. You won't always understand why certain decisions are being made given the complexity and flexibility of these algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "$y = w_1x+w_2$\n",
    "\n",
    "where $w_1$ is slope and $w_2$ is intercept. \n",
    "\n",
    "#### Absolute trick\n",
    "\n",
    "$y = (w_1+p\\alpha)x+(w_2+\\alpha)$  for coordinate (p,q) and $\\alpha$ = learning rate\n",
    "\n",
    "#### Square trick\n",
    "\n",
    "Comprehends distance from target. \n",
    "\n",
    "$y = (w_1+p(q-q')\\alpha)x+(w_2+(q-q')\\alpha)$  for coordinate (p,q) and $\\alpha$ = learning rate\n",
    "\n",
    "Let's say that we have a line whose equation is y = -0.6x + 4. For the point (x,y) = (-5, 3), apply the absolute trick to get the new equation for the line, using a learning rate of alpha = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=-0.550x+3.990\n",
      "y=-0.550x+3.990\n"
     ]
    }
   ],
   "source": [
    "w1 = -0.6 - (-5)*0.01\n",
    "w2 = 4 - 0.01\n",
    "print(\"y={:.3f}x+{:.3f}\".format(w1,w2))\n",
    "\n",
    "w3 = -0.6 + -5*(3-4)*0.01\n",
    "w4 = 4 + (3-4)*0.01\n",
    "print(\"y={:.3f}x+{:.3f}\".format(w3,w4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "$w_i -> w_i-\\alpha \\frac{\\delta}{\\delta w_i}Error$\n",
    "\n",
    "#### Mean Absolute Error\n",
    "\n",
    "Always want Error to be positive so it doesn't cancel out with other negative coefficients.\n",
    "\n",
    "$Error = \\frac{1}{m}\\sum^m_{i=1}|y-\\hat{y}|$\n",
    "\n",
    "#### Mean Squared Error\n",
    "\n",
    "$Error = \\frac{1}{2m}\\sum^m_{i=1}(y-\\hat{y})^2$\n",
    "\n",
    "Compute the mean absolute error and squared error for the following line and points:\n",
    "line: y = 1.2x + 2\n",
    "points: (2, -2), (5, 6), (-4, -4), (-7, 1), (8, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.88\n",
      "10.692000000000002\n"
     ]
    }
   ],
   "source": [
    "x = [2,5,-4,-7,8]\n",
    "y = [-2,6,-4,1,14]\n",
    "\n",
    "def mean_absolute_error(x, y):\n",
    "    y_h = [0]*len(x)\n",
    "    error = 0\n",
    "    for i in range(len(x)):\n",
    "        y_h[i] = 1.2*x[i]+2\n",
    "        error += abs(y[i] - y_h[i])\n",
    "    return error*(1/(len(x)))\n",
    "        \n",
    "print(mean_absolute_error(x, y))\n",
    "\n",
    "def mean_squared_error(x, y):\n",
    "    y_h = [0]*len(x)\n",
    "    error = 0\n",
    "    for i in range(len(x)):\n",
    "        y_h[i] = 1.2*x[i]+2\n",
    "        error += (y[i] - y_h[i])**2\n",
    "    return error*(1/(2*len(x)))\n",
    "\n",
    "print(mean_squared_error(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Error Functions\n",
    "\n",
    "Development of the derivative of the error function\n",
    "Notice that we've defined the squared error to be\n",
    "\n",
    "$Error = \\frac{1}{2} (y - \\hat{y})^2$\n",
    "\n",
    "Also, we've defined the prediction to be\n",
    "\n",
    "$\\hat{y} = w_1 x + w_2$ \n",
    "\n",
    "So to calculate the derivative of the Error with respect to $w_1$ we simply use the chain rule:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_1} Error = \\frac{\\partial Error}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_i}$\n",
    "\n",
    "The first factor of the right hand side is the derivative of the Error with respect to the prediction $\\hat{y}$ \n",
    "which is $-(y-\\hat{y})$. \n",
    "\n",
    "The second factor is the derivative of the prediction with respect to $w_1$ which is simply $x$.\n",
    "\n",
    "Therefore, the derivative is:\n",
    "\n",
    "$\\frac{\\partial}{\\partial w_1} Error = -(y-\\hat{y})x$   \n",
    "$\\frac{\\partial}{\\partial w_2} Error = -(y-\\hat{y})$\n",
    "\n",
    "Note for absolute error:   \n",
    "$\\frac{\\partial}{\\partial w_1} Error = \\pm x$   \n",
    "$\\frac{\\partial}{\\partial w_2} Error = \\pm 1$\n",
    "\n",
    "#### Gradient Step\n",
    "\n",
    "$w_i \\rightarrow w_i - \\alpha \\frac{\\partial}{\\partial w_i}Error$\n",
    "\n",
    "#### Mean vs Total Squared (or Absolute) Error   \n",
    "A potential confusion is the following: How do we know if we should use the mean or the total squared (or absolute) error?\n",
    "\n",
    "The total squared error is the sum of errors at each point, given by the following equation:\n",
    "\n",
    "$M = \\sum_{i=1}^m \\frac{1}{2} (y - \\hat{y})^2$\n",
    "\n",
    "whereas the mean squared error is the average of these errors, given by the equation, where $m$ is the number of points:\n",
    "\n",
    "$T = \\sum_{i=1}^m \\frac{1}{2m}(y - \\hat{y})^2$\n",
    "\n",
    "The good news is, it doesn't really matter. As we can see, the total squared error is just a multiple of the mean squared error, since\n",
    "\n",
    "$M = mT$\n",
    "\n",
    "Therefore, since derivatives are linear functions, the gradient of $T$ is also $m$ times the gradient of $M$.\n",
    "\n",
    "However, the gradient descent step consists of subtracting the gradient of the error times the learning rate $\\alpha$. Therefore, choosing between the mean squared error and the total squared error really just amounts to picking a different learning rate.\n",
    "\n",
    "In real life, we'll have algorithms that will help us determine a good learning rate to work with. Therefore, if we use the mean error or the total error, the algorithm will just end up picking a different learning rate.\n",
    "\n",
    "### Batch vs Stochastic Gradient Descent\n",
    "At this point, it seems that we've seen two ways of doing linear regression.\n",
    "\n",
    "By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    "By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "More specifically, the squared (or absolute) trick, when applied to a point, gives us some values to add to the weights of the model. We can add these values, update our weights, and then apply the squared (or absolute) trick on the next point. Or we can calculate these values for all the points, add them, and then update the weights with the sum of these values.\n",
    "\n",
    "The latter is called batch gradient descent. The former is called stochastic gradient descent. \n",
    "\n",
    "The question is, which one is used in practice?\n",
    "\n",
    "Actually, in most cases, neither. Think about this: If your data is huge, both are a bit slow, computationally. The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "`from sklearn.linear_model import LinearRegression  `   \n",
    "`model = LinearRegression()   `   \n",
    "`model.fit(x_values, y_values)   `   \n",
    "`print(model.predict([ [127], [248] ])) `    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
